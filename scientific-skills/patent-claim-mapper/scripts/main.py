#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Patent Claim Mapper - ä¸“åˆ©æƒåˆ©è¦æ±‚ä¾µæƒé£é™©åˆ†æå·¥å…·

åŠŸèƒ½ï¼š
1. è§£æä¸“åˆ©æƒåˆ©è¦æ±‚æ–‡æœ¬
2. æå–æŠ€æœ¯ç‰¹å¾
3. ä¸äº§å“ç‰¹å¾è¿›è¡Œæ¯”å¯¹åˆ†æ
4. ç”Ÿæˆä¾µæƒé£é™©è¯„ä¼°æŠ¥å‘Š

Author: OpenClaw Skills Team
Version: 1.0.0
"""

import re
import json
import argparse
from dataclasses import dataclass, field, asdict
from typing import List, Dict, Optional, Tuple
from pathlib import Path
from datetime import datetime


@dataclass
class ClaimElement:
    """æƒåˆ©è¦æ±‚ä¸­çš„æŠ€æœ¯è¦ç´ """
    text: str
    element_type: str  # 'apparatus', 'method', 'feature', 'limitation'
    keywords: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict:
        return asdict(self)


@dataclass
class PatentClaim:
    """ä¸“åˆ©æƒåˆ©è¦æ±‚"""
    number: int
    is_independent: bool
    parent_claim: Optional[int]  # ä»å±æƒåˆ©è¦æ±‚å¼•ç”¨
    text: str
    elements: List[ClaimElement] = field(default_factory=list)
    preamble: str = ""  # å‰åºéƒ¨åˆ†
    body: str = ""      # ç‰¹å¾éƒ¨åˆ†
    
    def to_dict(self) -> Dict:
        return {
            'number': self.number,
            'is_independent': self.is_independent,
            'parent_claim': self.parent_claim,
            'text': self.text,
            'preamble': self.preamble,
            'body': self.body,
            'elements': [e.to_dict() for e in self.elements]
        }


@dataclass
class RiskAssessment:
    """é£é™©è¯„ä¼°ç»“æœ"""
    claim_number: int
    risk_level: str  # 'high', 'medium', 'low'
    match_score: float  # 0-1
    matched_features: List[str] = field(default_factory=list)
    unmatched_features: List[str] = field(default_factory=list)
    analysis_notes: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict:
        return asdict(self)


class ClaimParser:
    """æƒåˆ©è¦æ±‚è§£æå™¨"""
    
    # æƒåˆ©è¦æ±‚ç¼–å·æ¨¡å¼
    CLAIM_NUMBER_PATTERN = re.compile(r'^(\d+)\.\s*')
    
    # ä»å±æƒåˆ©è¦æ±‚å¼•ç”¨æ¨¡å¼
    DEPENDENT_PATTERN = re.compile(
        r'æ ¹æ®|å¦‚|ä¾æ®.*?æƒåˆ©è¦æ±‚\s*(\d+)|å¼•ç”¨.*?æƒåˆ©è¦æ±‚\s*(\d+)',
        re.IGNORECASE | re.UNICODE
    )
    
    # å‰åºéƒ¨åˆ†å’Œç‰¹å¾éƒ¨åˆ†åˆ†éš”
    PREAMBLE_SEPARATORS = ['å…¶ç‰¹å¾åœ¨äº', 'å…¶ç‰¹å¾åœ¨äºï¼Œ', 'ç‰¹å¾æ˜¯', 'ç‰¹å¾æ˜¯ï¼Œ', 
                          'characterized in that', 'wherein', 'comprising:']
    
    def parse(self, text: str) -> List[PatentClaim]:
        """
        è§£ææƒåˆ©è¦æ±‚æ–‡æœ¬
        
        Args:
            text: æƒåˆ©è¦æ±‚ä¹¦å…¨æ–‡
            
        Returns:
            PatentClaimå¯¹è±¡åˆ—è¡¨
        """
        claims = []
        claim_texts = self._split_claims(text)
        
        for claim_text in claim_texts:
            claim = self._parse_single_claim(claim_text)
            if claim:
                claims.append(claim)
        
        # è§£æå„æƒåˆ©è¦æ±‚çš„æŠ€æœ¯è¦ç´ 
        for claim in claims:
            claim.elements = self._extract_elements(claim)
        
        return claims
    
    def _split_claims(self, text: str) -> List[str]:
        """å°†æƒåˆ©è¦æ±‚ä¹¦æ–‡æœ¬åˆ†å‰²ä¸ºå•ä¸ªæƒåˆ©è¦æ±‚"""
        # æŒ‰æ•°å­—ç¼–å·åˆ†å‰²ï¼ˆå¦‚ "1." "2."ï¼‰
        pattern = re.compile(r'\n\s*(\d+)\.\s+', re.MULTILINE)
        parts = pattern.split(text)
        
        claims = []
        for i in range(1, len(parts), 2):
            if i + 1 < len(parts):
                claim_num = parts[i]
                claim_content = parts[i + 1].strip()
                claims.append(f"{claim_num}. {claim_content}")
        
        return claims
    
    def _parse_single_claim(self, text: str) -> Optional[PatentClaim]:
        """è§£æå•ä¸ªæƒåˆ©è¦æ±‚"""
        # æå–æƒåˆ©è¦æ±‚ç¼–å·
        match = self.CLAIM_NUMBER_PATTERN.match(text)
        if not match:
            return None
        
        number = int(match.group(1))
        content = text[match.end():].strip()
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºç‹¬ç«‹æƒåˆ©è¦æ±‚
        parent_claim = self._extract_parent_claim(content)
        is_independent = parent_claim is None
        
        # åˆ†å‰²å‰åºéƒ¨åˆ†å’Œç‰¹å¾éƒ¨åˆ†
        preamble, body = self._split_preamble_body(content)
        
        return PatentClaim(
            number=number,
            is_independent=is_independent,
            parent_claim=parent_claim,
            text=content,
            preamble=preamble,
            body=body
        )
    
    def _extract_parent_claim(self, text: str) -> Optional[int]:
        """æå–ä»å±æƒåˆ©è¦æ±‚å¼•ç”¨çš„çˆ¶æƒåˆ©è¦æ±‚ç¼–å·"""
        match = self.DEPENDENT_PATTERN.search(text)
        if match:
            # è¿”å›ç¬¬ä¸€ä¸ªéç©ºçš„åŒ¹é…ç»„
            for group in match.groups():
                if group:
                    return int(group)
        return None
    
    def _split_preamble_body(self, text: str) -> Tuple[str, str]:
        """åˆ†å‰²å‰åºéƒ¨åˆ†å’Œç‰¹å¾éƒ¨åˆ†"""
        for separator in self.PREAMBLE_SEPARATORS:
            if separator in text:
                parts = text.split(separator, 1)
                return parts[0].strip(), parts[1].strip()
        
        # é»˜è®¤è¿”å›å…¨æ–‡ä½œä¸ºå‰åºéƒ¨åˆ†
        return text, ""
    
    def _extract_elements(self, claim: PatentClaim) -> List[ClaimElement]:
        """æå–æƒåˆ©è¦æ±‚ä¸­çš„æŠ€æœ¯è¦ç´ """
        elements = []
        
        # åˆ†å‰²ç‰¹å¾ï¼ˆé€šå¸¸ä»¥åˆ†å·ã€é€—å·+ä»¥åŠã€é€—å·+å¹¶ä¸”åˆ†éš”ï¼‰
        text_to_analyze = claim.body if claim.body else claim.text
        
        # ç‰¹å¾åˆ†éš”ç¬¦
        separators = r'ï¼›|ï¼›|ï¼›|\.|,\s*ä»¥åŠ|,\s*å¹¶ä¸”|,\s*and|,\s*wherein|;'
        parts = re.split(separators, text_to_analyze)
        
        for part in parts:
            part = part.strip()
            if len(part) > 5:  # è¿‡æ»¤è¿‡çŸ­çš„ç‰‡æ®µ
                keywords = self._extract_keywords(part)
                element = ClaimElement(
                    text=part,
                    element_type=self._classify_element(part),
                    keywords=keywords
                )
                elements.append(element)
        
        return elements
    
    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–ï¼šåè¯æ€§è¯ç»„
        # è¿™é‡Œä½¿ç”¨å¯å‘å¼è§„åˆ™ï¼Œå®é™…å¯æ¥å…¥NLPæ¨¡å‹
        keywords = []
        
        # æå–å¼•å·å†…çš„æœ¯è¯­
        quoted = re.findall(r'[""'']([^""'']+)[""'']', text)
        keywords.extend(quoted)
        
        # æå–å¤§å†™æœ¯è¯­ï¼ˆè‹±æ–‡ï¼‰
        capitalized = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', text)
        keywords.extend(capitalized)
        
        # æå–æŠ€æœ¯æœ¯è¯­ï¼ˆåŒ…å«ç‰¹å®šæŠ€æœ¯è¯çš„çŸ­è¯­ï¼‰
        tech_patterns = [
            r'\w+å™¨', r'\w+è£…ç½®', r'\w+ç³»ç»Ÿ', r'\w+æ–¹æ³•',
            r'\w+æ¨¡å—', r'\w+å•å…ƒ', r'\w+ç»„ä»¶',
            r'\w+device', r'\w+system', r'\w+method', r'\w+apparatus'
        ]
        for pattern in tech_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            keywords.extend(matches)
        
        return list(set(keywords))
    
    def _classify_element(self, text: str) -> str:
        """åˆ†ç±»æŠ€æœ¯è¦ç´ ç±»å‹"""
        text_lower = text.lower()
        
        if any(kw in text_lower for kw in ['è£…ç½®', 'è®¾å¤‡', 'ç³»ç»Ÿ', 'apparatus', 'device', 'system']):
            return 'apparatus'
        elif any(kw in text_lower for kw in ['æ–¹æ³•', 'æ­¥éª¤', 'æµç¨‹', 'method', 'process', 'step']):
            return 'method'
        elif any(kw in text_lower for kw in ['åŒ…æ‹¬', 'åŒ…å«', 'comprising', 'comprises', 'having']):
            return 'feature'
        else:
            return 'limitation'


class InfringementAnalyzer:
    """ä¾µæƒåˆ†æå™¨"""
    
    def __init__(self, risk_threshold: float = 0.7):
        self.risk_threshold = risk_threshold
    
    def analyze(self, claims: List[PatentClaim], 
                product_features: List[str]) -> List[RiskAssessment]:
        """
        åˆ†æä¾µæƒé£é™©
        
        Args:
            claims: ä¸“åˆ©æƒåˆ©è¦æ±‚åˆ—è¡¨
            product_features: äº§å“ç‰¹å¾æè¿°åˆ—è¡¨
            
        Returns:
            é£é™©è¯„ä¼°ç»“æœåˆ—è¡¨
        """
        assessments = []
        
        for claim in claims:
            assessment = self._analyze_single_claim(claim, product_features)
            assessments.append(assessment)
        
        return assessments
    
    def _analyze_single_claim(self, claim: PatentClaim, 
                              product_features: List[str]) -> RiskAssessment:
        """åˆ†æå•ä¸ªæƒåˆ©è¦æ±‚çš„ä¾µæƒé£é™©"""
        matched = []
        unmatched = []
        notes = []
        
        # æå–æƒåˆ©è¦æ±‚çš„æ‰€æœ‰æŠ€æœ¯ç‰¹å¾æ–‡æœ¬
        claim_features = [e.text for e in claim.elements]
        
        for claim_feature in claim_features:
            best_match = self._find_best_match(claim_feature, product_features)
            
            if best_match['score'] >= 0.6:
                matched.append({
                    'claim_feature': claim_feature,
                    'product_feature': best_match['feature'],
                    'similarity': best_match['score']
                })
            else:
                unmatched.append(claim_feature)
        
        # è®¡ç®—æ•´ä½“åŒ¹é…åº¦
        if claim_features:
            match_score = len(matched) / len(claim_features)
        else:
            match_score = 0.0
        
        # ç¡®å®šé£é™©ç­‰çº§
        risk_level = self._determine_risk_level(match_score, matched, unmatched)
        
        # ç”Ÿæˆåˆ†æè¯´æ˜
        notes = self._generate_analysis_notes(claim, matched, unmatched, match_score)
        
        return RiskAssessment(
            claim_number=claim.number,
            risk_level=risk_level,
            match_score=round(match_score, 2),
            matched_features=[m['claim_feature'] for m in matched],
            unmatched_features=unmatched,
            analysis_notes=notes
        )
    
    def _find_best_match(self, claim_feature: str, 
                         product_features: List[str]) -> Dict:
        """æ‰¾åˆ°ä¸æƒåˆ©è¦æ±‚ç‰¹å¾æœ€åŒ¹é…çš„äº§å“ç‰¹å¾"""
        best_score = 0.0
        best_feature = ""
        
        claim_keywords = set(self._extract_words(claim_feature))
        
        for product_feature in product_features:
            score = self._calculate_similarity(claim_feature, product_feature, claim_keywords)
            if score > best_score:
                best_score = score
                best_feature = product_feature
        
        return {'feature': best_feature, 'score': best_score}
    
    def _calculate_similarity(self, claim_feature: str, product_feature: str,
                              claim_keywords: set) -> float:
        """è®¡ç®—ä¸¤ä¸ªç‰¹å¾çš„ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰"""
        product_keywords = set(self._extract_words(product_feature))
        
        if not claim_keywords or not product_keywords:
            return 0.0
        
        # Jaccardç›¸ä¼¼åº¦
        intersection = len(claim_keywords & product_keywords)
        union = len(claim_keywords | product_keywords)
        
        jaccard = intersection / union if union > 0 else 0.0
        
        # åŒ…å«å…³ç³»åŠ åˆ†
        containment_bonus = 0.0
        if claim_feature.lower() in product_feature.lower():
            containment_bonus = 0.3
        elif product_feature.lower() in claim_feature.lower():
            containment_bonus = 0.2
        
        return min(1.0, jaccard + containment_bonus)
    
    def _extract_words(self, text: str) -> List[str]:
        """æå–æ–‡æœ¬ä¸­çš„å…³é”®è¯"""
        # å»é™¤æ ‡ç‚¹ï¼Œåˆ†è¯
        text = re.sub(r'[^\w\s]', ' ', text)
        words = text.lower().split()
        
        # è¿‡æ»¤åœç”¨è¯
        stopwords = {'çš„', 'æ˜¯', 'åœ¨', 'å’Œ', 'äº†', 'ä¸', 'æˆ–', 'the', 'a', 'an', 
                     'is', 'are', 'was', 'were', 'of', 'to', 'in', 'and', 'or'}
        
        return [w for w in words if w not in stopwords and len(w) > 1]
    
    def _determine_risk_level(self, match_score: float, matched: List, 
                              unmatched: List) -> str:
        """ç¡®å®šé£é™©ç­‰çº§"""
        if match_score >= self.risk_threshold:
            return 'high'
        elif match_score >= 0.4:
            return 'medium'
        else:
            return 'low'
    
    def _generate_analysis_notes(self, claim: PatentClaim, matched: List, 
                                  unmatched: List, score: float) -> List[str]:
        """ç”Ÿæˆåˆ†æè¯´æ˜"""
        notes = []
        
        if score >= self.risk_threshold:
            notes.append(f"æƒåˆ©è¦æ±‚{claim.number}ä¸äº§å“ç‰¹å¾é«˜åº¦åŒ¹é…ï¼Œå­˜åœ¨å®è´¨æ€§ä¾µæƒé£é™©")
        elif score >= 0.4:
            notes.append(f"æƒåˆ©è¦æ±‚{claim.number}ä¸äº§å“ç‰¹å¾éƒ¨åˆ†åŒ¹é…ï¼Œå»ºè®®è¿›ä¸€æ­¥è¯„ä¼°")
        else:
            notes.append(f"æƒåˆ©è¦æ±‚{claim.number}ä¸äº§å“ç‰¹å¾åŒ¹é…åº¦è¾ƒä½")
        
        if claim.is_independent:
            notes.append("è¯¥æƒåˆ©è¦æ±‚ä¸ºç‹¬ç«‹æƒåˆ©è¦æ±‚ï¼Œä¿æŠ¤èŒƒå›´æœ€å¹¿ï¼Œéœ€é‡ç‚¹å…³æ³¨")
        else:
            notes.append(f"è¯¥æƒåˆ©è¦æ±‚ä¸ºä»å±æƒåˆ©è¦æ±‚ï¼Œå¼•ç”¨æƒåˆ©è¦æ±‚{claim.parent_claim}")
        
        if matched:
            notes.append(f"å·²åŒ¹é…{len(matched)}é¡¹æŠ€æœ¯ç‰¹å¾")
        if unmatched:
            notes.append(f"æœªåŒ¹é…{len(unmatched)}é¡¹æŠ€æœ¯ç‰¹å¾ï¼Œå¯èƒ½ä¸ºè§„é¿è®¾è®¡ç‚¹")
        
        return notes


class ReportGenerator:
    """æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    RISK_ICONS = {
        'high': 'ğŸ”´',
        'medium': 'ğŸŸ¡',
        'low': 'ğŸŸ¢'
    }
    
    RISK_LABELS = {
        'high': 'é«˜é£é™©',
        'medium': 'ä¸­é£é™©',
        'low': 'ä½é£é™©'
    }
    
    def generate(self, assessments: List[RiskAssessment], 
                 patent_info: Dict, product_name: str) -> str:
        """ç”Ÿæˆé£é™©è¯„ä¼°æŠ¥å‘Š"""
        report = []
        
        # æŠ¥å‘Šæ ‡é¢˜
        report.append("# ä¸“åˆ©ä¾µæƒé£é™©åˆ†ææŠ¥å‘Š")
        report.append("")
        
        # åŸºæœ¬ä¿¡æ¯
        report.append("## åŸºæœ¬ä¿¡æ¯")
        report.append(f"- **åˆ†ææ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
        report.append(f"- **ç›®æ ‡äº§å“**: {product_name}")
        if patent_info.get('patent_number'):
            report.append(f"- **ä¸“åˆ©å·**: {patent_info['patent_number']}")
        if patent_info.get('patent_title'):
            report.append(f"- **ä¸“åˆ©åç§°**: {patent_info['patent_title']}")
        report.append("")
        
        # é£é™©æ¦‚è§ˆ
        report.append("## é£é™©æ¦‚è§ˆ")
        report.append("")
        report.append("| æƒåˆ©è¦æ±‚ | ç±»å‹ | é£é™©ç­‰çº§ | åŒ¹é…åº¦ |")
        report.append("|---------|------|---------|-------|")
        
        for assessment in assessments:
            claim_type = "ç‹¬ç«‹" if assessment.claim_number == 1 or not assessment.claim_number else "ä»å±"
            icon = self.RISK_ICONS.get(assessment.risk_level, 'âšª')
            label = self.RISK_LABELS.get(assessment.risk_level, 'æœªçŸ¥')
            report.append(
                f"| æƒåˆ©è¦æ±‚{assessment.claim_number} | {claim_type} | "
                f"{icon} {label} | {assessment.match_score * 100:.0f}% |"
            )
        report.append("")
        
        # è¯¦ç»†åˆ†æ
        report.append("## è¯¦ç»†åˆ†æ")
        report.append("")
        
        for assessment in assessments:
            report.extend(self._generate_claim_analysis(assessment))
            report.append("")
        
        # ç»“è®ºä¸å»ºè®®
        report.append("## ç»“è®ºä¸å»ºè®®")
        report.append("")
        
        high_risks = [a for a in assessments if a.risk_level == 'high']
        medium_risks = [a for a in assessments if a.risk_level == 'medium']
        
        if high_risks:
            report.append("### âš ï¸ é«˜é£é™©è­¦å‘Š")
            report.append(f"å‘ç°{len(high_risks)}é¡¹æƒåˆ©è¦æ±‚å­˜åœ¨é«˜é£é™©ï¼š")
            for risk in high_risks:
                report.append(f"- æƒåˆ©è¦æ±‚{risk.claim_number}ï¼ˆåŒ¹é…åº¦{risk.match_score * 100:.0f}%ï¼‰")
            report.append("")
            report.append("**å»ºè®®æªæ–½**:")
            report.append("1. ç«‹å³è¿›è¡Œè¯¦ç»†çš„FTOåˆ†æ")
            report.append("2. è€ƒè™‘è®¾è®¡è§„é¿æ–¹æ¡ˆ")
            report.append("3. å’¨è¯¢ä¸“ä¸šä¸“åˆ©å¾‹å¸ˆ")
            report.append("")
        
        if medium_risks:
            report.append("### ğŸ” ä¸­ç­‰é£é™©æç¤º")
            report.append(f"å‘ç°{len(medium_risks)}é¡¹æƒåˆ©è¦æ±‚éœ€è¦å…³æ³¨")
            report.append("")
        
        report.append("### è§„é¿è®¾è®¡å»ºè®®")
        report.append("é’ˆå¯¹æœªåŒ¹é…çš„æŠ€æœ¯ç‰¹å¾ï¼Œå¯è€ƒè™‘ä»¥ä¸‹è§„é¿æ–¹å‘ï¼š")
        
        for assessment in assessments:
            if assessment.unmatched_features:
                report.append(f"\n**æƒåˆ©è¦æ±‚{assessment.claim_number}**:")
                for feature in assessment.unmatched_features[:3]:  # æœ€å¤šæ˜¾ç¤º3ä¸ª
                    report.append(f"- è€ƒè™‘ä¿®æ”¹æˆ–ç§»é™¤: {feature[:50]}...")
        
        report.append("")
        report.append("---")
        report.append("*å…è´£å£°æ˜ï¼šæœ¬æŠ¥å‘ŠåŸºäºæ–‡æœ¬ç›¸ä¼¼åº¦ç®—æ³•è‡ªåŠ¨ç”Ÿæˆï¼Œä»…ä¾›æŠ€æœ¯å‚è€ƒï¼Œä¸æ„æˆæ³•å¾‹æ„è§ã€‚æœ€ç»ˆä¾µæƒåˆ¤å®šè¯·å’¨è¯¢ä¸“ä¸šä¸“åˆ©å¾‹å¸ˆã€‚*")
        
        return "\n".join(report)
    
    def _generate_claim_analysis(self, assessment: RiskAssessment) -> List[str]:
        """ç”Ÿæˆå•ä¸ªæƒåˆ©è¦æ±‚çš„åˆ†æ"""
        lines = []
        
        icon = self.RISK_ICONS.get(assessment.risk_level, 'âšª')
        lines.append(f"### æƒåˆ©è¦æ±‚{assessment.claim_number} {icon}")
        lines.append("")
        
        lines.append(f"**é£é™©ç­‰çº§**: {self.RISK_LABELS.get(assessment.risk_level, 'æœªçŸ¥')}")
        lines.append(f"**åŒ¹é…åº¦**: {assessment.match_score * 100:.0f}%")
        lines.append("")
        
        if assessment.analysis_notes:
            lines.append("**åˆ†æè¯´æ˜**:")
            for note in assessment.analysis_notes:
                lines.append(f"- {note}")
            lines.append("")
        
        if assessment.matched_features:
            lines.append("**å·²åŒ¹é…æŠ€æœ¯ç‰¹å¾**:")
            for feature in assessment.matched_features[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                lines.append(f"- âœ… {feature[:80]}...")
            lines.append("")
        
        if assessment.unmatched_features:
            lines.append("**æœªåŒ¹é…æŠ€æœ¯ç‰¹å¾ï¼ˆæ½œåœ¨è§„é¿ç‚¹ï¼‰**:")
            for feature in assessment.unmatched_features[:5]:
                lines.append(f"- âš ï¸ {feature[:80]}...")
            lines.append("")
        
        return lines
    
    def generate_json(self, assessments: List[RiskAssessment], 
                      patent_info: Dict, product_name: str) -> str:
        """ç”ŸæˆJSONæ ¼å¼æŠ¥å‘Š"""
        data = {
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'product_name': product_name,
                'patent_info': patent_info
            },
            'summary': {
                'total_claims': len(assessments),
                'high_risk': len([a for a in assessments if a.risk_level == 'high']),
                'medium_risk': len([a for a in assessments if a.risk_level == 'medium']),
                'low_risk': len([a for a in assessments if a.risk_level == 'low'])
            },
            'assessments': [a.to_dict() for a in assessments]
        }
        return json.dumps(data, ensure_ascii=False, indent=2)


class PatentClaimMapper:
    """ä¸“åˆ©æƒåˆ©è¦æ±‚æ˜ å°„åˆ†æä¸»ç±»"""
    
    def __init__(self, config: Optional[Dict] = None):
        """
        åˆå§‹åŒ–
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config or {}
        self.parser = ClaimParser()
        self.analyzer = InfringementAnalyzer(
            risk_threshold=self.config.get('risk_threshold', 0.7)
        )
        self.report_generator = ReportGenerator()
    
    def load_claims(self, file_path: str) -> List[PatentClaim]:
        """
        ä»æ–‡ä»¶åŠ è½½æƒåˆ©è¦æ±‚
        
        Args:
            file_path: æƒåˆ©è¦æ±‚ä¹¦æ–‡ä»¶è·¯å¾„
            
        Returns:
            PatentClaimå¯¹è±¡åˆ—è¡¨
        """
        path = Path(file_path)
        if not path.exists():
            raise FileNotFoundError(f"æƒåˆ©è¦æ±‚æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        text = path.read_text(encoding='utf-8')
        return self.parser.parse(text)
    
    def load_claims_from_text(self, text: str) -> List[PatentClaim]:
        """ä»æ–‡æœ¬ç›´æ¥åŠ è½½æƒåˆ©è¦æ±‚"""
        return self.parser.parse(text)
    
    def analyze_infringement(self, claims: List[PatentClaim],
                            product_features: List[str]) -> List[RiskAssessment]:
        """
        åˆ†æä¾µæƒé£é™©
        
        Args:
            claims: ä¸“åˆ©æƒåˆ©è¦æ±‚åˆ—è¡¨
            product_features: äº§å“ç‰¹å¾åˆ—è¡¨
            
        Returns:
            é£é™©è¯„ä¼°ç»“æœåˆ—è¡¨
        """
        return self.analyzer.analyze(claims, product_features)
    
    def generate_report(self, assessments: List[RiskAssessment],
                       patent_info: Dict = None,
                       product_name: str = "æœªå‘½åäº§å“") -> str:
        """
        ç”Ÿæˆåˆ†ææŠ¥å‘Š
        
        Args:
            assessments: é£é™©è¯„ä¼°ç»“æœ
            patent_info: ä¸“åˆ©ä¿¡æ¯å­—å…¸
            product_name: äº§å“åç§°
            
        Returns:
            æŠ¥å‘Šæ–‡æœ¬ï¼ˆMarkdownæ ¼å¼ï¼‰
        """
        patent_info = patent_info or {}
        return self.report_generator.generate(assessments, patent_info, product_name)
    
    def generate_json_report(self, assessments: List[RiskAssessment],
                            patent_info: Dict = None,
                            product_name: str = "æœªå‘½åäº§å“") -> str:
        """ç”ŸæˆJSONæ ¼å¼æŠ¥å‘Š"""
        patent_info = patent_info or {}
        return self.report_generator.generate_json(assessments, patent_info, product_name)
    
    def run_full_analysis(self, patent_file: str, product_file: str,
                         output_file: str = None) -> str:
        """
        è¿è¡Œå®Œæ•´çš„åˆ†ææµç¨‹
        
        Args:
            patent_file: ä¸“åˆ©æƒåˆ©è¦æ±‚ä¹¦æ–‡ä»¶è·¯å¾„
            product_file: äº§å“ç‰¹å¾æè¿°æ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            åˆ†ææŠ¥å‘Šæ–‡æœ¬
        """
        # åŠ è½½æƒåˆ©è¦æ±‚
        claims = self.load_claims(patent_file)
        print(f"âœ… å·²è§£æ {len(claims)} é¡¹æƒåˆ©è¦æ±‚")
        
        # åŠ è½½äº§å“ç‰¹å¾
        product_text = Path(product_file).read_text(encoding='utf-8')
        product_features = self._extract_product_features(product_text)
        print(f"âœ… å·²æå– {len(product_features)} é¡¹äº§å“ç‰¹å¾")
        
        # åˆ†æä¾µæƒé£é™©
        assessments = self.analyze_infringement(claims, product_features)
        
        # ç”ŸæˆæŠ¥å‘Š
        patent_info = {'patent_number': Path(patent_file).stem}
        product_name = Path(product_file).stem
        report = self.generate_report(assessments, patent_info, product_name)
        
        # ä¿å­˜æŠ¥å‘Š
        if output_file:
            Path(output_file).write_text(report, encoding='utf-8')
            print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_file}")
        
        return report
    
    def _extract_product_features(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬æå–äº§å“ç‰¹å¾"""
        features = []
        
        # æŒ‰è¡Œåˆ†å‰²
        lines = text.strip().split('\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # å»é™¤åˆ—è¡¨æ ‡è®°
            for prefix in ['-', '*', 'â€¢', '1.', '2.', '3.', '4.', '5.']:
                if line.startswith(prefix):
                    line = line[len(prefix):].strip()
            
            if len(line) > 5:
                features.append(line)
        
        # å¦‚æœè¡Œæ•°å¤ªå°‘ï¼Œå°è¯•æŒ‰å¥å­åˆ†å‰²
        if len(features) < 3:
            sentences = re.split(r'[ã€‚\.\n]+', text)
            features = [s.strip() for s in sentences if len(s.strip()) > 10]
        
        return features


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description='Patent Claim Mapper - ä¸“åˆ©æƒåˆ©è¦æ±‚ä¾µæƒé£é™©åˆ†æå·¥å…·'
    )
    
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # analyze å‘½ä»¤
    analyze_parser = subparsers.add_parser('analyze', help='åˆ†æä¾µæƒé£é™©')
    analyze_parser.add_argument('--patent', '-p', required=True,
                               help='ä¸“åˆ©æƒåˆ©è¦æ±‚ä¹¦æ–‡ä»¶è·¯å¾„')
    analyze_parser.add_argument('--product', '-pr', required=True,
                               help='äº§å“ç‰¹å¾æè¿°æ–‡ä»¶è·¯å¾„')
    analyze_parser.add_argument('--output', '-o',
                               help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„')
    analyze_parser.add_argument('--format', '-f', choices=['markdown', 'json'],
                               default='markdown', help='è¾“å‡ºæ ¼å¼')
    
    # parse å‘½ä»¤
    parse_parser = subparsers.add_parser('parse', help='ä»…è§£ææƒåˆ©è¦æ±‚ç»“æ„')
    parse_parser.add_argument('--patent', '-p', required=True,
                             help='ä¸“åˆ©æƒåˆ©è¦æ±‚ä¹¦æ–‡ä»¶è·¯å¾„')
    parse_parser.add_argument('--output', '-o',
                             help='è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    if args.command == 'analyze':
        mapper = PatentClaimMapper()
        
        try:
            report = mapper.run_full_analysis(
                args.patent,
                args.product,
                args.output
            )
            if not args.output:
                print("\n" + "=" * 60)
                print(report)
        except Exception as e:
            print(f"âŒ åˆ†æå¤±è´¥: {e}")
            return 1
    
    elif args.command == 'parse':
        mapper = PatentClaimMapper()
        
        try:
            claims = mapper.load_claims(args.patent)
            print(f"âœ… æˆåŠŸè§£æ {len(claims)} é¡¹æƒåˆ©è¦æ±‚")
            
            # æ‰“å°æƒåˆ©è¦æ±‚ç»“æ„
            for claim in claims:
                print(f"\n{'=' * 40}")
                print(f"æƒåˆ©è¦æ±‚ {claim.number}")
                print(f"ç±»å‹: {'ç‹¬ç«‹' if claim.is_independent else 'ä»å±'}")
                if claim.parent_claim:
                    print(f"å¼•ç”¨: æƒåˆ©è¦æ±‚ {claim.parent_claim}")
                print(f"å‰åºéƒ¨åˆ†: {claim.preamble[:100]}...")
                print(f"ç‰¹å¾éƒ¨åˆ†: {claim.body[:100]}...")
                print(f"æŠ€æœ¯è¦ç´ æ•°é‡: {len(claim.elements)}")
            
            if args.output:
                data = {'claims': [c.to_dict() for c in claims]}
                Path(args.output).write_text(
                    json.dumps(data, ensure_ascii=False, indent=2),
                    encoding='utf-8'
                )
                print(f"\nâœ… å·²ä¿å­˜è‡³: {args.output}")
        
        except Exception as e:
            print(f"âŒ è§£æå¤±è´¥: {e}")
            return 1
    
    else:
        parser.print_help()
    
    return 0


if __name__ == '__main__':
    exit(main())
