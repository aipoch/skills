# Score Calibration Guide

## Overview

This guide helps reviewers assign consistent, appropriate scores by providing reference points and calibration exercises.

---

## Score Calibration Principles

### 1. Use the Full Scale

The NIH 1-9 scale should be used fully:
- Score 1-2: Reserved for truly exceptional applications (~top 5%)
- Score 3-4: Excellent applications with minor issues (~next 15%)
- Score 5-6: Good applications with weaknesses (~middle 30%)
- Score 7-8: Fair to marginal applications with major issues (~next 35%)
- Score 9: Poor applications with fatal flaws (~bottom 15%)

**Common Error**: Score compression in the 4-6 range. Reviewers tend to avoid the extremes.

### 2. Score Relative to the Field

Consider what is typical in the specific research area:
- A "good" study in a mature field may differ from a "good" study in an emerging area
- Standards for methodology vary by discipline
- Sample sizes should be appropriate for the field

### 3. Consistency Across Applications

In a single review session:
- Apply the same standards to all applications
- Calibrate against other applications in the same session
- Consider the distribution of scores across the meeting

---

## Score Calibration Exercises

### Exercise 1: Significance Calibration

**Scenario A**: Application proposes to sequence 100 genomes of a rare disease with no existing genetic studies. Could identify first disease genes.
- **Calibrated Score**: 1-2 (First-in-field, transformative potential)

**Scenario B**: Application proposes to study a well-characterized pathway in a common disease using standard methods. Incremental advance expected.
- **Calibrated Score**: 5-6 (Incremental, limited novelty)

**Scenario C**: Application proposes to repeat a published study with minor modifications. No new insights expected.
- **Calibrated Score**: 8-9 (Trivial significance)

### Exercise 2: Approach Calibration

**Scenario A**: Well-powered clinical trial with appropriate controls, comprehensive analysis plan, extensive preliminary data, and detailed alternatives.
- **Calibrated Score**: 1-2 (Rigorous, feasible, well-supported)

**Scenario B**: Reasonable experimental plan but sample size calculations missing and limited preliminary data for one aim.
- **Calibrated Score**: 4-5 (Sound but with gaps)

**Scenario C**: Three ambitious aims with no preliminary data, no power calculations, and no discussion of pitfalls.
- **Calibrated Score**: 7-8 (Weak approach, unlikely to succeed)

### Exercise 3: Innovation Calibration

**Scenario A**: Proposes to apply CRISPR screening to a disease where no systematic genetic screens exist. New method + new application.
- **Calibrated Score**: 1-3 (Novel application of powerful technology)

**Scenario B**: Uses established methods to ask a new question. Methods are standard but application is new.
- **Calibrated Score**: 4-5 (Some innovation in application)

**Scenario C**: Direct extension of published work with same methods, same system, slight variation.
- **Calibrated Score**: 7-8 (Little to no innovation)

---

## Score Adjustment Guidelines

### When to Adjust Down (Better Score)

| Situation | Adjustment |
|-----------|------------|
| Truly exceptional preliminary data | -1 to -2 |
| Paradigm-shifting potential | -1 to -2 |
| Distinguished investigator with perfect match | -1 |
| Unique resources or population access | -0 to -1 |
| Elegant integration of multiple approaches | -0 to -1 |

### When to Adjust Up (Worse Score)

| Situation | Adjustment |
|-----------|------------|
| Fatal flaw in approach | +2 to +3 |
| Investigator clearly unqualified | +2 to +3 |
| Aims too ambitious for timeframe | +1 to +2 |
| Insufficient preliminary data | +1 to +2 |
| Missing key controls | +1 |
| Statistical plan inadequate | +1 |
| No pitfalls/alternatives discussed | +1 |

---

## Common Scoring Biases

### 1. Central Tendency Bias
**Problem**: Clustering all scores around 4-5
**Solution**: Force use of full scale; compare to other applications

### 2. Halo Effect
**Problem**: One strong criterion inflates all others
**Solution**: Score each criterion independently before viewing others

### 3. Leniency Bias
**Problem**: Consistently scoring higher to be "nice"
**Solution**: Remember scores determine funding; inaccurate scores hurt applicants

### 4. Severity Bias
**Problem**: Consistently harsh scoring
**Solution**: Calibrate against study section norms; look for strengths

### 5. Prestige Bias
**Problem**: Scoring based on institution rather than merit
**Solution**: Blind review when possible; focus on proposal content

### 6. Order Effects
**Problem**: First or last applications scored differently
**Solution**: Re-calibrate after every few applications

---

## Score Verification Checklist

Before finalizing a score, verify:

- [ ] Can I articulate specific reasons for this score?
- [ ] Is this score consistent with how I've scored similar applications?
- [ ] Would another reviewer likely assign a similar score?
- [ ] Does the critique support the assigned score?
- [ ] Have I considered both strengths and weaknesses?
- [ ] Am I using the full 1-9 scale appropriately?
- [ ] Is this score appropriate for the field?

---

## Score Distribution Guidelines

In a typical study section reviewing 50-100 applications:

| Score Range | Expected Percentage | Notes |
|-------------|---------------------|-------|
| 1-2 | 5-10% | Truly exceptional |
| 3-4 | 15-20% | Very good, likely fundable |
| 5-6 | 25-30% | Good, borderline |
| 7-8 | 30-35% | Fair, unlikely to fund |
| 9 | 10-15% | Poor, not fundable |

If your distribution differs significantly, re-calibrate.

---

## Calibration Against Fundability

### Definitely Fundable (Payline < 20%)
- Overall Impact: 1-3
- No criterion > 5
- Approach: 1-3

### Probably Fundable (Payline ~20-30%)
- Overall Impact: 3-4
- No criterion > 6
- Approach: 2-4

### Borderline (Payline ~30-40%)
- Overall Impact: 4-5
- One criterion may be 6-7 if others are strong
- Approach: 3-5

### Unlikely to Fund (Payline > 40%)
- Overall Impact: 6+
- Major weakness in Approach or Significance
- Multiple criteria > 6

---

## Self-Assessment Questions

After assigning scores, ask yourself:

1. **Would I fund this?** If yes, score should probably be 1-4. If no, 5-9.

2. **How does this compare to funded grants I know?** Calibrate against real examples.

3. **Am I being swayed by presentation quality?** Focus on scientific merit, not writing polish.

4. **Would my score change if I reviewed this tomorrow?** If yes, reconsider.

5. **Am I confident in each score?** If uncertain, re-read the relevant section.

---

## Final Calibration Step

Before submitting reviews:

1. Review all your assigned scores
2. Verify the distribution roughly matches expected
3. Ensure your best application is scored 1-2
4. Ensure your worst application is scored 8-9
5. Verify no score compression in the middle
6. Confirm critique justifies every score
