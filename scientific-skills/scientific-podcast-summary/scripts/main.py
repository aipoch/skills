#!/usr/bin/env python3
"""
Scientific Podcast Summary - è‡ªåŠ¨æ€»ç»“ç§‘å­¦æ’­å®¢å†…å®¹
æ”¯æŒ: Huberman Lab, Nature Podcast
"""

import argparse
import json
import os
import re
import sys
from datetime import datetime
from typing import Optional
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup


# ==================== Configuration ====================

DEFAULT_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")

PODCAST_SOURCES = {
    "huberman": {
        "name": "Huberman Lab",
        "base_url": "https://hubermanlab.com",
        "latest_url": "https://hubermanlab.com/category/podcast-episodes/",
    },
    "nature": {
        "name": "Nature Podcast",
        "base_url": "https://www.nature.com",
        "latest_url": "https://www.nature.com/nature/articles?type=podcast",
    },
}

SUMMARY_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç§‘å­¦æ’­å®¢å†…å®¹æ€»ç»“åŠ©æ‰‹ã€‚è¯·å¯¹ä»¥ä¸‹æ’­å®¢å†…å®¹è¿›è¡Œç»“æ„åŒ–æ€»ç»“ã€‚

è¦æ±‚ï¼š
1. æå–æ ¸å¿ƒç§‘å­¦ä¸»é¢˜å’Œå…³é”®å‘ç°
2. ä½¿ç”¨ç®€æ´æ¸…æ™°çš„è¯­è¨€
3. ä¿ç•™é‡è¦çš„ä¸“ä¸šæœ¯è¯­å¹¶é€‚å½“è§£é‡Š
4. çªå‡ºå®ç”¨çš„å»ºè®®æˆ–è¡ŒåŠ¨æŒ‡å—

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{
    "title": "æ’­å®¢æ ‡é¢˜",
    "publish_date": "å‘å¸ƒæ—¥æœŸ",
    "host": "ä¸»æŒäºº",
    "guests": ["å˜‰å®¾1", "å˜‰å®¾2"],
    "summary": "æ ¸å¿ƒä¸»é¢˜æ¦‚è¿° (200-300å­—)",
    "key_points": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"],
    "actionable_tips": ["å»ºè®®1", "å»ºè®®2"],
    "resources": [{"title": "èµ„æºå", "url": "é“¾æ¥"}]
}

æ’­å®¢å†…å®¹ï¼š
{content}
"""


# ==================== Utils ====================

def log(msg: str, level: str = "info"):
    """æ‰“å°æ—¥å¿—"""
    prefix = {"info": "â„¹ï¸", "success": "âœ…", "error": "âŒ", "warn": "âš ï¸"}.get(level, "â„¹ï¸")
    print(f"{prefix} {msg}", file=sys.stderr if level == "error" else sys.stdout)


def fetch_url(url: str, headers: Optional[dict] = None) -> Optional[str]:
    """è·å–URLå†…å®¹"""
    default_headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
    }
    if headers:
        default_headers.update(headers)
    
    try:
        resp = requests.get(url, headers=default_headers, timeout=30)
        resp.raise_for_status()
        return resp.text
    except Exception as e:
        log(f"è·å–URLå¤±è´¥: {url} - {e}", "error")
        return None


def call_llm(prompt: str) -> Optional[str]:
    """è°ƒç”¨LLM API"""
    if not OPENAI_API_KEY:
        log("æœªè®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡", "error")
        return None
    
    try:
        import openai
        client = openai.OpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_BASE_URL,
        )
        
        resp = client.chat.completions.create(
            model=DEFAULT_MODEL,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç§‘å­¦å†…å®¹æ€»ç»“åŠ©æ‰‹ã€‚"},
                {"role": "user", "content": prompt},
            ],
            temperature=0.3,
        )
        return resp.choices[0].message.content
    except Exception as e:
        log(f"LLM APIè°ƒç”¨å¤±è´¥: {e}", "error")
        return None


# ==================== Podcast Parsers ====================

def parse_huberman_episode(html: str, url: str) -> dict:
    """è§£æ Huberman Lab é¡µé¢"""
    soup = BeautifulSoup(html, "html.parser")
    
    # æå–æ ‡é¢˜
    title_elem = soup.find("h1", class_=re.compile("entry-title|post-title"))
    title = title_elem.get_text(strip=True) if title_elem else "Unknown"
    
    # æå–å‘å¸ƒæ—¥æœŸ
    date_elem = soup.find("time", class_="entry-date")
    publish_date = date_elem.get("datetime", "") if date_elem else ""
    
    # æå–å†…å®¹
    content_elem = soup.find("div", class_=re.compile("entry-content|post-content"))
    content = ""
    if content_elem:
        # ç§»é™¤è„šæœ¬å’Œæ ·å¼
        for script in content_elem(["script", "style"]):
            script.decompose()
        content = content_elem.get_text(separator="\n", strip=True)
    
    # æå–å˜‰å®¾ä¿¡æ¯ (é€šå¸¸åœ¨æ ‡é¢˜æˆ–å†…å®¹ä¸­)
    guests = []
    guest_match = re.search(r"Dr\.\s+([A-Z][a-z]+\s+[A-Z][a-z]+)", title)
    if guest_match:
        guests.append(guest_match.group(0))
    
    return {
        "title": title,
        "publish_date": publish_date,
        "host": "Andrew Huberman",
        "guests": guests,
        "content": content[:15000],  # é™åˆ¶é•¿åº¦
        "source_url": url,
    }


def parse_nature_podcast(html: str, url: str) -> dict:
    """è§£æ Nature Podcast é¡µé¢"""
    soup = BeautifulSoup(html, "html.parser")
    
    # æå–æ ‡é¢˜
    title_elem = soup.find("h1") or soup.find("h2", class_=re.compile("title"))
    title = title_elem.get_text(strip=True) if title_elem else "Unknown"
    
    # æå–å‘å¸ƒæ—¥æœŸ
    date_elem = soup.find("time") or soup.find("span", class_=re.compile("date"))
    publish_date = date_elem.get_text(strip=True) if date_elem else ""
    
    # æå–å†…å®¹
    content_elem = soup.find("div", class_=re.compile("article-body|content"))
    content = ""
    if content_elem:
        for script in content_elem(["script", "style"]):
            script.decompose()
        content = content_elem.get_text(separator="\n", strip=True)
    
    return {
        "title": title,
        "publish_date": publish_date,
        "host": "Nature Podcast",
        "guests": [],
        "content": content[:15000],
        "source_url": url,
    }


def parse_generic_page(html: str, url: str) -> dict:
    """é€šç”¨é¡µé¢è§£æ"""
    soup = BeautifulSoup(html, "html.parser")
    
    # å°è¯•æå–æ ‡é¢˜
    title = "Unknown"
    for selector in ["h1", "h2", "title"]:
        elem = soup.find(selector)
        if elem:
            title = elem.get_text(strip=True)
            break
    
    # æå–æ­£æ–‡å†…å®¹
    content = ""
    for selector in ["article", "main", ".content", "#content", ".post"]:
        elem = soup.find(selector)
        if elem:
            content = elem.get_text(separator="\n", strip=True)
            break
    
    if not content:
        # é€€å›åˆ°æå–æ‰€æœ‰æ®µè½
        paragraphs = soup.find_all("p")
        content = "\n\n".join(p.get_text(strip=True) for p in paragraphs[:20])
    
    return {
        "title": title,
        "publish_date": "",
        "host": "",
        "guests": [],
        "content": content[:15000],
        "source_url": url,
    }


# ==================== Feed Discovery ====================

def get_latest_huberman_url() -> Optional[str]:
    """è·å–æœ€æ–° Huberman Lab å‰§é›†URL"""
    html = fetch_url(PODCAST_SOURCES["huberman"]["latest_url"])
    if not html:
        return None
    
    soup = BeautifulSoup(html, "html.parser")
    
    # æŸ¥æ‰¾æœ€æ–°å‰§é›†é“¾æ¥
    link = soup.find("a", href=re.compile(r"/\d{4}/\d{2}/\d{2}/"))
    if link:
        return link.get("href")
    
    # å¤‡é€‰æ–¹æ¡ˆ
    for article in soup.find_all("article"):
        link = article.find("a", href=True)
        if link:
            href = link.get("href")
            if "/" in href:
                return urljoin(PODCAST_SOURCES["huberman"]["base_url"], href)
    
    return None


def get_latest_nature_url() -> Optional[str]:
    """è·å–æœ€æ–° Nature Podcast å‰§é›†URL"""
    html = fetch_url(PODCAST_SOURCES["nature"]["latest_url"])
    if not html:
        return None
    
    soup = BeautifulSoup(html, "html.parser")
    
    # æŸ¥æ‰¾æœ€æ–°podcasté“¾æ¥
    for link in soup.find_all("a", href=True):
        href = link.get("href", "")
        if "/nature/articles/" in href:
            return urljoin(PODCAST_SOURCES["nature"]["base_url"], href)
    
    return None


# ==================== Summary Generation ====================

def generate_summary(episode_data: dict) -> dict:
    """ä½¿ç”¨LLMç”Ÿæˆæ€»ç»“"""
    prompt = SUMMARY_PROMPT.format(content=episode_data["content"])
    
    response = call_llm(prompt)
    if not response:
        log("LLMç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æå–", "warn")
        return fallback_summary(episode_data)
    
    # è§£æJSONå“åº”
    try:
        # å°è¯•æå–JSONå—
        json_match = re.search(r'\{[\s\S]*\}', response)
        if json_match:
            summary_data = json.loads(json_match.group())
            summary_data["source_url"] = episode_data.get("source_url", "")
            return summary_data
    except json.JSONDecodeError:
        pass
    
    # å¦‚æœJSONè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å“åº”
    return {
        "title": episode_data.get("title", "Unknown"),
        "publish_date": episode_data.get("publish_date", ""),
        "host": episode_data.get("host", ""),
        "guests": episode_data.get("guests", []),
        "summary": response[:500],
        "key_points": [],
        "actionable_tips": [],
        "resources": [{"title": "åŸæ–‡é“¾æ¥", "url": episode_data.get("source_url", "")}],
        "source_url": episode_data.get("source_url", ""),
    }


def fallback_summary(episode_data: dict) -> dict:
    """LLMå¤±è´¥æ—¶çš„åŸºç¡€æå–"""
    content = episode_data.get("content", "")
    
    # ç®€å•æå–å‰å‡ ä¸ªæ®µè½ä½œä¸ºå…³é”®è¦ç‚¹
    paragraphs = [p.strip() for p in content.split("\n\n") if len(p.strip()) > 50][:5]
    
    return {
        "title": episode_data.get("title", "Unknown"),
        "publish_date": episode_data.get("publish_date", ""),
        "host": episode_data.get("host", ""),
        "guests": episode_data.get("guests", []),
        "summary": paragraphs[0] if paragraphs else "",
        "key_points": paragraphs[1:4] if len(paragraphs) > 1 else [],
        "actionable_tips": [],
        "resources": [{"title": "åŸæ–‡é“¾æ¥", "url": episode_data.get("source_url", "")}],
        "source_url": episode_data.get("source_url", ""),
    }


# ==================== Output Formatters ====================

def format_markdown(summary: dict) -> str:
    """æ ¼å¼åŒ–ä¸º Markdown"""
    lines = [
        f"# ğŸ™ï¸ {summary['title']}",
        "",
        f"**å‘å¸ƒæ—¶é—´:** {summary.get('publish_date', 'N/A')}",
        f"**ä¸»æŒäºº:** {summary.get('host', 'N/A')}",
    ]
    
    if summary.get('guests'):
        lines.append(f"**å˜‰å®¾:** {', '.join(summary['guests'])}")
    
    lines.extend(["", "---", ""])
    
    # æ ¸å¿ƒä¸»é¢˜
    lines.extend(["## ğŸ“ æ ¸å¿ƒä¸»é¢˜", ""])
    lines.append(summary.get('summary', 'æš‚æ— æ¦‚è¿°'))
    lines.append("")
    
    # å…³é”®è¦ç‚¹
    if summary.get('key_points'):
        lines.extend(["## ğŸ”¬ å…³é”®è¦ç‚¹", ""])
        for i, point in enumerate(summary['key_points'], 1):
            lines.append(f"{i}. {point}")
        lines.append("")
    
    # å®ç”¨å»ºè®®
    if summary.get('actionable_tips'):
        lines.extend(["## ğŸ’¡ å®ç”¨å»ºè®®", ""])
        for tip in summary['actionable_tips']:
            lines.append(f"- {tip}")
        lines.append("")
    
    # èµ„æºé“¾æ¥
    if summary.get('resources'):
        lines.extend(["## ğŸ“š ç›¸å…³èµ„æº", ""])
        for res in summary['resources']:
            title = res.get('title', 'é“¾æ¥')
            url = res.get('url', '#')
            lines.append(f"- [{title}]({url})")
        lines.append("")
    
    lines.extend(["---", f"\n*ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}*"])
    
    return "\n".join(lines)


def format_json(summary: dict) -> str:
    """æ ¼å¼åŒ–ä¸º JSON"""
    summary["generated_at"] = datetime.now().isoformat()
    return json.dumps(summary, ensure_ascii=False, indent=2)


# ==================== Main ====================

def main():
    parser = argparse.ArgumentParser(
        description="è‡ªåŠ¨æ€»ç»“ç§‘å­¦æ’­å®¢å†…å®¹ (Huberman Lab / Nature Podcast)"
    )
    parser.add_argument(
        "--podcast",
        choices=["huberman", "nature"],
        default="huberman",
        help="é€‰æ‹©æ’­å®¢æº (é»˜è®¤: huberman)",
    )
    parser.add_argument(
        "--url",
        help="ç›´æ¥æä¾›æ’­å®¢é¡µé¢URL",
    )
    parser.add_argument(
        "--output", "-o",
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--format",
        choices=["markdown", "json"],
        default="markdown",
        help="è¾“å‡ºæ ¼å¼ (é»˜è®¤: markdown)",
    )
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—",
    )
    
    args = parser.parse_args()
    
    # è·å–ç›®æ ‡URL
    target_url = args.url
    if not target_url:
        log(f"æ­£åœ¨è·å–æœ€æ–° {PODCAST_SOURCES[args.podcast]['name']} å‰§é›†...")
        if args.podcast == "huberman":
            target_url = get_latest_huberman_url()
        else:
            target_url = get_latest_nature_url()
    
    if not target_url:
        log("æ— æ³•è·å–æ’­å®¢URL", "error")
        sys.exit(1)
    
    log(f"è§£æé¡µé¢: {target_url}")
    
    # è·å–é¡µé¢å†…å®¹
    html = fetch_url(target_url)
    if not html:
        sys.exit(1)
    
    # è§£æå†…å®¹
    if args.podcast == "huberman":
        episode_data = parse_huberman_episode(html, target_url)
    elif args.podcast == "nature":
        episode_data = parse_nature_podcast(html, target_url)
    else:
        episode_data = parse_generic_page(html, target_url)
    
    if not episode_data.get("content"):
        log("æ— æ³•æå–é¡µé¢å†…å®¹", "error")
        sys.exit(1)
    
    log(f"æå–å†…å®¹é•¿åº¦: {len(episode_data['content'])} å­—ç¬¦")
    
    # ç”Ÿæˆæ€»ç»“
    log("æ­£åœ¨ç”ŸæˆAIæ€»ç»“...")
    summary = generate_summary(episode_data)
    
    # æ ¼å¼åŒ–è¾“å‡º
    if args.format == "json":
        output = format_json(summary)
    else:
        output = format_markdown(summary)
    
    # è¾“å‡ºç»“æœ
    if args.output:
        with open(args.output, "w", encoding="utf-8") as f:
            f.write(output)
        log(f"å·²ä¿å­˜åˆ°: {args.output}", "success")
    else:
        print(output)


if __name__ == "__main__":
    main()
